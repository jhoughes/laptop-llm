{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Create Ollama models directory (persist container), Run Ollama, Install small LLM (llama3.2:1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mkdir -p ~/local_llm_lab/ollama_models\n",
    "\n",
    "docker volume create --opt type=none --opt o=bind --opt device=~/local_llm_lab/ollama_models ollama_models\n",
    "\n",
    "docker run -d -v ollama_models:/root/.ollama -p 11435:11434 --name ollama ollama/ollama\n",
    "\n",
    "docker exec ollama ollama run llama3.2:1b\n",
    "\n",
    "docker exec ollama ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Destroy container, show that LLMs persist (no download due to Docker volume), start Ollama chat in CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "docker stop ollama\n",
    "\n",
    "docker rm ollama\n",
    "\n",
    "docker ps -a\n",
    "\n",
    "docker container ls -a\n",
    "\n",
    "docker volume list\n",
    "\n",
    "docker run -d -v ollama_models:/root/.ollama -p 11435:11434 --name ollama ollama/ollama\n",
    "\n",
    "docker exec ollama ollama run llama3.2:1b\n",
    "\n",
    "docker exec ollama ollama ps\n",
    "\n",
    "docker exec -it ollama ollama run llama3.2:1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Run Open WebUI for GUI frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Open port 3000 in browser and do quick runthrough of Open WebUI; check running models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 - Add new LLM, show models in Open WebUI, plus quick GUI Ollama chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "docker exec ollama ollama run moondream\n",
    "\n",
    "docker exec ollama ollama pull gemma3:1b\n",
    "\n",
    "docker exec ollama ollama ps\n",
    "\n",
    "docker exec ollama ollama ls\n",
    "\n",
    "#Check models in Open WebUI"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
